"""
è¯­æ–™æ ‡æ³¨æ¨¡å—æ¼”ç¤ºè„šæœ¬

å±•ç¤ºDataMaxè¯­æ–™æ ‡æ³¨æ¨¡å—çš„å®Œæ•´åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- æ–‡æœ¬åˆ†å‰²
- QAç”Ÿæˆ
- æ•°æ®å¯¼å‡º
- æ‰¹é‡å¤„ç†

ä½¿ç”¨æ–¹æ³•:
    python -m datamax.labeler.demo
"""

import json
from pathlib import Path
from typing import List

from loguru import logger

from .base_labeler import LabelConfig
from .llm_client import BaseLLMClient
from .prompt_manager import PromptManager
from .qa_labeler import QALabeler, QAConfig
from .text_splitter import TextSplitter, SplitterConfig
from .token_counter import TokenCounter


class DemoLLMClient(BaseLLMClient):
    """æ¼”ç¤ºç”¨çš„æ¨¡æ‹ŸLLMå®¢æˆ·ç«¯"""
    
    def __init__(self):
        self.call_count = 0
    
    def chat(self, messages):
        """æ¨¡æ‹Ÿå¯¹è¯å“åº”"""
        self.call_count += 1
        
        # æ¨¡æ‹Ÿä¸åŒç±»å‹çš„å“åº”
        if self.call_count % 3 == 1:
            return """
            1. é—®é¢˜ï¼šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ
               ç­”æ¡ˆï¼šäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯ä¸€é—¨ç ”ç©¶å¦‚ä½•è®©æœºå™¨å…·å¤‡ç±»ä¼¼äººç±»æ™ºèƒ½çš„å­¦ç§‘ï¼ŒåŒ…æ‹¬å­¦ä¹ ã€æ¨ç†ã€æ„ŸçŸ¥ç­‰èƒ½åŠ›ã€‚
               
            2. é—®é¢˜ï¼šæœºå™¨å­¦ä¹ çš„ä¸»è¦ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ
               ç­”æ¡ˆï¼šæœºå™¨å­¦ä¹ æ˜¯AIçš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ï¼Œå®ƒè®©è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ è§„å¾‹ï¼Œè€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚
               
            3. é—®é¢˜ï¼šæ·±åº¦å­¦ä¹ æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ
               ç­”æ¡ˆï¼šæ·±åº¦å­¦ä¹ ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥è§£å†³å¤æ‚é—®é¢˜ï¼Œåœ¨å›¾åƒè¯†åˆ«å’Œè‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚
            """
        elif self.call_count % 3 == 2:
            return """
            1. é—®é¢˜ï¼šNLPæŠ€æœ¯çš„åº”ç”¨é¢†åŸŸæœ‰å“ªäº›ï¼Ÿ
               ç­”æ¡ˆï¼šNLPæŠ€æœ¯è®©è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ï¼Œå¹¿æ³›åº”ç”¨äºæœç´¢å¼•æ“ã€ç¿»è¯‘è½¯ä»¶ã€æ™ºèƒ½å®¢æœç­‰é¢†åŸŸã€‚
               
            2. é—®é¢˜ï¼šäººå·¥æ™ºèƒ½åŒ…å«å“ªäº›å­é¢†åŸŸï¼Ÿ
               ç­”æ¡ˆï¼šäººå·¥æ™ºèƒ½åŒ…æ‹¬æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ã€ä¸“å®¶ç³»ç»Ÿç­‰å¤šä¸ªé‡è¦å­é¢†åŸŸã€‚
            """
        else:
            return """
            1. é—®é¢˜ï¼šä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œï¼Ÿ
               ç­”æ¡ˆï¼šç¥ç»ç½‘ç»œæ˜¯æ¨¡æ‹Ÿäººè„‘ç¥ç»å…ƒè¿æ¥æ–¹å¼çš„è®¡ç®—æ¨¡å‹ï¼Œç”±å¤šä¸ªäº’è”çš„èŠ‚ç‚¹ç»„æˆï¼Œèƒ½å¤Ÿå­¦ä¹ å¤æ‚çš„æ•°æ®æ¨¡å¼ã€‚
               
            2. é—®é¢˜ï¼šæ•°æ®åœ¨æœºå™¨å­¦ä¹ ä¸­çš„é‡è¦æ€§å¦‚ä½•ï¼Ÿ
               ç­”æ¡ˆï¼šæ•°æ®æ˜¯æœºå™¨å­¦ä¹ çš„åŸºç¡€ï¼Œé«˜è´¨é‡çš„æ•°æ®èƒ½å¸®åŠ©æ¨¡å‹å­¦ä¹ åˆ°æ›´å‡†ç¡®çš„è§„å¾‹ï¼Œæ˜¯AIç³»ç»ŸæˆåŠŸçš„å…³é”®å› ç´ ã€‚
            """
    
    async def achat(self, messages):
        """å¼‚æ­¥å¯¹è¯å“åº”"""
        return self.chat(messages)
    
    def count_tokens(self, text: str) -> int:
        """ä¼°ç®—tokenæ•°é‡"""
        return len(text) // 3


def create_sample_documents():
    """åˆ›å»ºç¤ºä¾‹æ–‡æ¡£"""
    documents = [
        {
            "title": "äººå·¥æ™ºèƒ½åŸºç¡€",
            "content": """
            äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„æœºå™¨å’Œè½¯ä»¶ç³»ç»Ÿã€‚
            
            ## å‘å±•å†ç¨‹
            äººå·¥æ™ºèƒ½çš„æ¦‚å¿µæœ€æ—©å¯ä»¥è¿½æº¯åˆ°1950å¹´ä»£ï¼Œå½“æ—¶è®¡ç®—æœºç§‘å­¦å®¶å¼€å§‹æ¢ç´¢æœºå™¨æ˜¯å¦èƒ½å¤Ÿ"æ€è€ƒ"ã€‚
            ç»è¿‡å‡ åå¹´çš„å‘å±•ï¼ŒAIæŠ€æœ¯å·²ç»åœ¨å„ä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚
            
            ## ä¸»è¦æŠ€æœ¯
            ç°ä»£AIä¸»è¦åŒ…æ‹¬æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ç­‰æŠ€æœ¯åˆ†æ”¯ã€‚
            è¿™äº›æŠ€æœ¯çš„ç»“åˆä½¿å¾—AIç³»ç»Ÿèƒ½å¤Ÿå¤„ç†è¶Šæ¥è¶Šå¤æ‚çš„ä»»åŠ¡ã€‚
            
            ## åº”ç”¨å‰æ™¯
            AIæŠ€æœ¯æ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»å’Œå·¥ä½œæ–¹å¼ï¼Œä»æ™ºèƒ½æ‰‹æœºåˆ°è‡ªåŠ¨é©¾é©¶æ±½è½¦ï¼Œ
            ä»åŒ»ç–—è¯Šæ–­åˆ°é‡‘èåˆ†æï¼ŒAIçš„åº”ç”¨å‰æ™¯ååˆ†å¹¿é˜”ã€‚
            """
        },
        {
            "title": "æœºå™¨å­¦ä¹ è¯¦è§£", 
            "content": """
            æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹ä»æ•°æ®ä¸­å­¦ä¹ ã€‚
            
            ## å­¦ä¹ æ–¹å¼
            æœºå™¨å­¦ä¹ ä¸»è¦åˆ†ä¸ºä¸‰ç§å­¦ä¹ æ–¹å¼ï¼š
            1. ç›‘ç£å­¦ä¹ ï¼šä½¿ç”¨æ ‡è®°æ•°æ®è¿›è¡Œè®­ç»ƒ
            2. æ— ç›‘ç£å­¦ä¹ ï¼šä»æœªæ ‡è®°æ•°æ®ä¸­å‘ç°æ¨¡å¼
            3. å¼ºåŒ–å­¦ä¹ ï¼šé€šè¿‡ä¸ç¯å¢ƒäº¤äº’å­¦ä¹ æœ€ä¼˜ç­–ç•¥
            
            ## ç®—æ³•ç±»å‹
            å¸¸è§çš„æœºå™¨å­¦ä¹ ç®—æ³•åŒ…æ‹¬çº¿æ€§å›å½’ã€å†³ç­–æ ‘ã€éšæœºæ£®æ—ã€æ”¯æŒå‘é‡æœºã€ç¥ç»ç½‘ç»œç­‰ã€‚
            ä¸åŒçš„ç®—æ³•é€‚ç”¨äºä¸åŒç±»å‹çš„é—®é¢˜å’Œæ•°æ®ã€‚
            
            ## å®é™…åº”ç”¨
            æœºå™¨å­¦ä¹ åœ¨æ¨èç³»ç»Ÿã€å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸéƒ½æœ‰å¹¿æ³›åº”ç”¨ã€‚
            """
        },
        {
            "title": "æ·±åº¦å­¦ä¹ é©å‘½",
            "content": """
            æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å­¦ä¹ è¿‡ç¨‹ã€‚
            
            ## æŠ€æœ¯åŸç†
            æ·±åº¦å­¦ä¹ é€šè¿‡æ„å»ºå¤šå±‚çš„äººå·¥ç¥ç»ç½‘ç»œï¼Œæ¯ä¸€å±‚éƒ½èƒ½æå–å’Œè½¬æ¢è¾“å…¥æ•°æ®çš„ç‰¹å¾ã€‚
            è¿™ç§å±‚æ¬¡åŒ–çš„ç‰¹å¾å­¦ä¹ ä½¿å¾—æ·±åº¦å­¦ä¹ èƒ½å¤Ÿå¤„ç†éå¸¸å¤æ‚çš„æ•°æ®æ¨¡å¼ã€‚
            
            ## çªç ´æ€§è¿›å±•
            æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ï¼Œ
            åœ¨æŸäº›ä»»åŠ¡ä¸Šç”šè‡³è¶…è¶Šäº†äººç±»çš„è¡¨ç°ã€‚
            
            ## è®¡ç®—è¦æ±‚
            æ·±åº¦å­¦ä¹ é€šå¸¸éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œæ•°æ®ï¼ŒGPUå’Œä¸“ç”¨èŠ¯ç‰‡çš„å‘å±•ä¸ºæ·±åº¦å­¦ä¹ æä¾›äº†å¼ºå¤§çš„è®¡ç®—æ”¯æŒã€‚
            """
        }
    ]
    
    return documents


def demo_text_splitting():
    """æ¼”ç¤ºæ–‡æœ¬åˆ†å‰²åŠŸèƒ½"""
    logger.info("ğŸ”ª å¼€å§‹æ¼”ç¤ºæ–‡æœ¬åˆ†å‰²åŠŸèƒ½...")
    
    # åˆ›å»ºåˆ†å‰²å™¨é…ç½®
    config = SplitterConfig(
        chunk_size=300,
        chunk_overlap=50,
        keep_separator=True
    )
    
    # æµ‹è¯•é€’å½’åˆ†å‰²å™¨
    recursive_splitter = TextSplitter.create_splitter("recursive", config)
    
    sample_text = """
    äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ã€‚å®ƒæ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„æœºå™¨ã€‚
    
    æœºå™¨å­¦ä¹ æ˜¯AIçš„æ ¸å¿ƒæŠ€æœ¯ã€‚å®ƒè®©è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ è§„å¾‹ã€‚
    é€šè¿‡è®­ç»ƒï¼Œæœºå™¨å­¦ä¹ æ¨¡å‹å¯ä»¥å¯¹æ–°æ•°æ®è¿›è¡Œé¢„æµ‹å’Œåˆ†ç±»ã€‚
    
    æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸã€‚å®ƒä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥è§£å†³å¤æ‚é—®é¢˜ã€‚
    åœ¨å›¾åƒè¯†åˆ«å’Œè‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢ï¼Œæ·±åº¦å­¦ä¹ å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚
    """
    
    chunks = recursive_splitter.split_text(sample_text)
    logger.info(f"é€’å½’åˆ†å‰²å™¨ç”Ÿæˆäº† {len(chunks)} ä¸ªæ–‡æœ¬å—")
    
    for i, chunk in enumerate(chunks):
        logger.info(f"å— {i+1}: {chunk.content[:50]}...")
    
    # æµ‹è¯•è¯­ä¹‰åˆ†å‰²å™¨
    semantic_splitter = TextSplitter.create_splitter("semantic", config)
    semantic_chunks = semantic_splitter.split_text(sample_text)
    logger.info(f"è¯­ä¹‰åˆ†å‰²å™¨ç”Ÿæˆäº† {len(semantic_chunks)} ä¸ªæ–‡æœ¬å—")


def demo_qa_generation():
    """æ¼”ç¤ºQAç”ŸæˆåŠŸèƒ½"""
    logger.info("ğŸ¤– å¼€å§‹æ¼”ç¤ºQAç”ŸæˆåŠŸèƒ½...")
    
    # åˆ›å»ºæ¨¡æ‹ŸLLMå®¢æˆ·ç«¯
    llm_client = DemoLLMClient()
    
    # åˆ›å»ºQAé…ç½®
    qa_config = QAConfig(
        num_qa_per_chunk=3,
        question_types=["factual", "comprehension", "application"],
        difficulty_levels=["easy", "medium", "hard"],
        enable_filtering=True
    )
    
    # åˆ›å»ºQAæ ‡æ³¨å™¨
    qa_labeler = QALabeler(llm_client, qa_config)
    
    # ç”ŸæˆQAå¯¹
    sample_texts = [
        "äººå·¥æ™ºèƒ½æ˜¯ä¸€é—¨ç ”ç©¶å¦‚ä½•è®©æœºå™¨å…·å¤‡ç±»ä¼¼äººç±»æ™ºèƒ½çš„å­¦ç§‘ã€‚å®ƒåŒ…æ‹¬æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰å¤šä¸ªå­é¢†åŸŸã€‚",
        "æœºå™¨å­¦ä¹ è®©è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ è§„å¾‹ï¼Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚å®ƒæ˜¯ç°ä»£AIç³»ç»Ÿçš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ã€‚"
    ]
    
    all_results = []
    for i, text in enumerate(sample_texts):
        logger.info(f"å¤„ç†æ–‡æœ¬ {i+1}...")
        results = qa_labeler.label_text(text)
        all_results.extend(results)
        
        for result in results:
            logger.info(f"æˆåŠŸç”Ÿæˆ {len(result.qa_pairs)} ä¸ªQAå¯¹")
    
    return all_results


def demo_batch_processing():
    """æ¼”ç¤ºæ‰¹é‡å¤„ç†åŠŸèƒ½"""
    logger.info("ğŸ“¦ å¼€å§‹æ¼”ç¤ºæ‰¹é‡å¤„ç†åŠŸèƒ½...")
    
    # åˆ›å»ºç¤ºä¾‹æ–‡æ¡£
    documents = create_sample_documents()
    
    # ä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶
    temp_files = []
    for i, doc in enumerate(documents):
        file_path = Path(f"temp_doc_{i+1}.txt")
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(f"# {doc['title']}\n\n{doc['content']}")
        temp_files.append(file_path)
    
    try:
        # åˆ›å»ºQAæ ‡æ³¨å™¨
        llm_client = DemoLLMClient()
        qa_labeler = QALabeler(llm_client)
        
        # æ‰¹é‡å¤„ç†æ–‡ä»¶
        all_results = []
        for file_path in temp_files:
            logger.info(f"å¤„ç†æ–‡ä»¶: {file_path}")
            results = qa_labeler.generate_qa_from_file(str(file_path))
            all_results.extend(results)
            logger.info(f"ä»æ–‡ä»¶ç”Ÿæˆäº† {len(results)} ä¸ªQAç»“æœ")
        
        return all_results
        
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        for file_path in temp_files:
            if file_path.exists():
                file_path.unlink()


def demo_data_export():
    """æ¼”ç¤ºæ•°æ®å¯¼å‡ºåŠŸèƒ½"""
    logger.info("ğŸ’¾ å¼€å§‹æ¼”ç¤ºæ•°æ®å¯¼å‡ºåŠŸèƒ½...")
    
    # ç”Ÿæˆä¸€äº›QAæ•°æ®
    llm_client = DemoLLMClient()
    qa_labeler = QALabeler(llm_client)
    
    sample_text = """
    æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å­¦ä¹ è¿‡ç¨‹ã€‚
    é€šè¿‡å±‚æ¬¡åŒ–çš„ç‰¹å¾å­¦ä¹ ï¼Œæ·±åº¦å­¦ä¹ èƒ½å¤Ÿå¤„ç†éå¸¸å¤æ‚çš„æ•°æ®æ¨¡å¼ï¼Œ
    åœ¨å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸéƒ½å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚
    """
    
    results = qa_labeler.label_text(sample_text)
    
    # å¯¼å‡ºä¸ºä¸åŒæ ¼å¼
    formats = ["jsonl", "json", "csv"]
    exported_files = []
    
    for format_type in formats:
        output_file = f"demo_output.{format_type}"
        try:
            qa_labeler.export_qa_dataset(results, output_file, format_type)
            exported_files.append(output_file)
            logger.info(f"æˆåŠŸå¯¼å‡ºä¸º {format_type.upper()} æ ¼å¼: {output_file}")
            
            # å±•ç¤ºå¯¼å‡ºå†…å®¹çš„é¢„è§ˆ
            if format_type == "jsonl":
                with open(output_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    logger.info(f"JSONLæ–‡ä»¶åŒ…å« {len(lines)} è¡Œæ•°æ®")
                    if lines:
                        sample_data = json.loads(lines[0])
                        logger.info(f"ç¤ºä¾‹QAå¯¹: {sample_data['question'][:50]}...")
                        
        except Exception as e:
            logger.error(f"å¯¼å‡º {format_type} æ ¼å¼å¤±è´¥: {str(e)}")
    
    return exported_files


def demo_statistics():
    """æ¼”ç¤ºç»Ÿè®¡åŠŸèƒ½"""
    logger.info("ğŸ“Š å¼€å§‹æ¼”ç¤ºç»Ÿè®¡åŠŸèƒ½...")
    
    # åˆ›å»ºtokenè®¡æ•°å™¨
    token_counter = TokenCounter()
    
    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ã€‚",
        "æœºå™¨å­¦ä¹ è®©è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ã€‚",
        "æ·±åº¦å­¦ä¹ ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œå¤„ç†å¤æ‚é—®é¢˜ã€‚"
    ]
    
    total_tokens = 0
    for i, text in enumerate(test_texts):
        token_count = token_counter.count_tokens(text)
        total_tokens += token_count
        logger.info(f"æ–‡æœ¬ {i+1} åŒ…å« {token_count} ä¸ªtokens")
    
    logger.info(f"æ€»å…± {total_tokens} ä¸ªtokens")
    
    # æ¼”ç¤ºç»Ÿè®¡ä¿¡æ¯
    avg_tokens = total_tokens / len(test_texts)
    logger.info(f"å¹³å‡æ¯ä¸ªæ–‡æœ¬ {avg_tokens:.1f} tokens")


def main():
    """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
    logger.info("ğŸš€ DataMaxè¯­æ–™æ ‡æ³¨æ¨¡å—æ¼”ç¤ºå¼€å§‹...")
    
    try:
        # 1. æ–‡æœ¬åˆ†å‰²æ¼”ç¤º
        demo_text_splitting()
        
        # 2. QAç”Ÿæˆæ¼”ç¤º
        qa_results = demo_qa_generation()
        
        # 3. æ‰¹é‡å¤„ç†æ¼”ç¤º
        batch_results = demo_batch_processing()
        
        # 4. æ•°æ®å¯¼å‡ºæ¼”ç¤º
        exported_files = demo_data_export()
        
        # 5. ç»Ÿè®¡åŠŸèƒ½æ¼”ç¤º
        demo_statistics()
        
        # æ€»ç»“
        logger.info("ğŸ“ˆ æ¼”ç¤ºæ€»ç»“:")
        logger.info(f"- ç”Ÿæˆäº† {len(qa_results)} ä¸ªQAç»“æœï¼ˆå•ç‹¬å¤„ç†ï¼‰")
        logger.info(f"- ç”Ÿæˆäº† {len(batch_results)} ä¸ªQAç»“æœï¼ˆæ‰¹é‡å¤„ç†ï¼‰")
        logger.info(f"- å¯¼å‡ºäº† {len(exported_files)} ä¸ªæ–‡ä»¶")
        
        logger.info("ğŸ‰ DataMaxè¯­æ–™æ ‡æ³¨æ¨¡å—æ¼”ç¤ºå®Œæˆï¼")
        
        # æ¸…ç†æ¼”ç¤ºæ–‡ä»¶
        cleanup_files = ["demo_output.jsonl", "demo_output.json", "demo_output.csv"]
        for file_path in cleanup_files:
            path = Path(file_path)
            if path.exists():
                path.unlink()
                logger.info(f"æ¸…ç†æ¼”ç¤ºæ–‡ä»¶: {file_path}")
        
    except Exception as e:
        logger.error(f"æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        raise


if __name__ == "__main__":
    main() 