"""
æ–‡æ¡£è½¬æ¢æ€§èƒ½å¯¹æ¯”ç¤ºä¾‹ - UNO API vs ä¼ ç»Ÿæ–¹å¼

è¿™ä¸ªç¤ºä¾‹å¯¹æ¯”äº†ä¸¤ç§æ–‡æ¡£è½¬æ¢æ–¹å¼çš„æ€§èƒ½å·®å¼‚ï¼š

1. ä¼ ç»Ÿæ–¹å¼ï¼ˆuse_uno=Falseï¼‰ï¼š
   - ç›´æ¥è°ƒç”¨ soffice --headless --convert-to å‘½ä»¤
   - æ¯æ¬¡è°ƒç”¨éƒ½æ˜¯ç‹¬ç«‹è¿›ç¨‹
   - æ— çŠ¶æ€ç®¡ç†ï¼Œå¼€é”€ç›¸å¯¹è¾ƒå°

2. UNO APIæ–¹å¼ï¼ˆuse_uno=Trueï¼‰ï¼š
   - ä½¿ç”¨LibreOfficeçš„UNO API
   - ç»´æŠ¤é•¿è¿æ¥å’ŒæœåŠ¡çŠ¶æ€
   - æ”¯æŒç²¾ç»†çš„æ–‡æ¡£æ§åˆ¶ï¼Œä½†æœ‰é¢å¤–å¼€é”€

æ€§èƒ½å·®å¼‚åŸå› ï¼š
- UNOæ–¹å¼æ…¢ï¼šLibreOfficeé‡é‡çº§ã€IPCé€šä¿¡ã€æœåŠ¡ç®¡ç†å¼€é”€
- ä¼ ç»Ÿæ–¹å¼å¿«ï¼šç›´æ¥å‘½ä»¤è°ƒç”¨ã€æ— ä¸­é—´å±‚ã€ç‹¬ç«‹è¿›ç¨‹

è¿è¡Œæ­¤ç¤ºä¾‹ä¼šè¿›è¡Œè¯¦ç»†çš„æ€§èƒ½æµ‹è¯•å’Œåˆ†æã€‚
"""

import os
import sys

# ç¡®ä¿å¯¼å…¥æœ¬åœ°å¼€å‘ç‰ˆæœ¬è€Œä¸æ˜¯å·²å®‰è£…çš„ç‰ˆæœ¬
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import asyncio
import concurrent.futures
import time
from pathlib import Path

# é…ç½®æ—¥å¿—
from loguru import logger

from datamax.parser.doc_parser import DocParser
from datamax.parser.docx_parser import DocxParser
from datamax.parser.ppt_parser import PPtParser
from datamax.utils import (
    HAS_UNO, 
    cleanup_uno_managers, 
    get_uno_manager, 
    pre_create_uno_managers, 
    warmup_uno_managers,
    release_uno_manager,
    uno_manager_context,
    get_uno_pool
)


def warmup_thread_pool(executor: concurrent.futures.ThreadPoolExecutor, num_tasks: int = None):
    """é¢„çƒ­çº¿ç¨‹æ± ï¼Œè®©æ‰€æœ‰çº¿ç¨‹çœŸæ­£å¯åŠ¨èµ·æ¥"""
    if num_tasks is None:
        # é»˜è®¤ä¸ºçº¿ç¨‹æ± çš„æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
        num_tasks = executor._max_workers
    
    def dummy_task(x):
        """ç®€å•çš„å ä½ä»»åŠ¡"""
        time.sleep(0.001)  # çŸ­æš‚ä¼‘çœ 
        return x * 2
    
    # æäº¤ä»»åŠ¡è®©çº¿ç¨‹å¯åŠ¨
    futures = [executor.submit(dummy_task, i) for i in range(num_tasks)]
    
    # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
    for future in concurrent.futures.as_completed(futures):
        _ = future.result()
    
    logger.debug(f"   âš¡ çº¿ç¨‹æ± é¢„çƒ­å®Œæˆï¼Œ{num_tasks}ä¸ªçº¿ç¨‹å·²å°±ç»ª")


def convert_document(file_path: str, use_uno: bool = True):
    """è½¬æ¢å•ä¸ªæ–‡æ¡£"""
    file_path = Path(file_path)

    start_time = time.time()

    try:
        if file_path.suffix.lower() == ".doc":
            parser = DocParser(str(file_path), use_uno=use_uno)
            result = parser.parse(str(file_path))
        elif file_path.suffix.lower() == ".docx":
            parser = DocxParser(str(file_path), use_uno=use_uno)
            result = parser.parse(str(file_path))
        elif file_path.suffix.lower() == ".ppt":
            parser = PPtParser(str(file_path), use_uno=use_uno)
            result = parser.parse(str(file_path))
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}")

        elapsed_time = time.time() - start_time
        logger.info(f"âœ… è½¬æ¢æˆåŠŸ: {file_path.name} (è€—æ—¶: {elapsed_time:.2f}ç§’)")
        
        # é‡Šæ”¾UNOç®¡ç†å™¨ï¼ˆå¦‚æœä½¿ç”¨ï¼‰
        if use_uno:
            release_uno_manager()
            
        return result

    except Exception as e:
        elapsed_time = time.time() - start_time
        logger.error(f"âŒ è½¬æ¢å¤±è´¥: {file_path.name} - {str(e)} (è€—æ—¶: {elapsed_time:.2f}ç§’)")
        # ç¡®ä¿é‡Šæ”¾ç®¡ç†å™¨
        if use_uno:
            release_uno_manager()
        raise


def batch_convert_sequential(file_paths: list, use_uno: bool = False):
    """é¡ºåºè½¬æ¢å¤šä¸ªæ–‡æ¡£ï¼ˆä¼ ç»Ÿæ–¹å¼ï¼‰"""
    logger.info(f"ğŸ”„ å¼€å§‹é¡ºåºè½¬æ¢ {len(file_paths)} ä¸ªæ–‡æ¡£...")
    start_time = time.time()

    results = []
    for file_path in file_paths:
        try:
            result = convert_document(file_path, use_uno=use_uno)
            results.append(result)
        except Exception as e:
            logger.error(f"è½¬æ¢å¤±è´¥: {file_path} - {str(e)}")

    total_time = time.time() - start_time
    logger.info(f"â±ï¸ é¡ºåºè½¬æ¢å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
    return results


def batch_convert_parallel(
    file_paths: list, max_workers: int = 4, use_uno: bool = True
):
    """å¹¶è¡Œè½¬æ¢å¤šä¸ªæ–‡æ¡£ï¼ˆä½¿ç”¨UNO APIï¼‰"""
    if not HAS_UNO and use_uno:
        logger.warning("âš ï¸ UNO API ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼")
        use_uno = False

    logger.info(f"ğŸš€ å¼€å§‹å¹¶è¡Œè½¬æ¢ {len(file_paths)} ä¸ªæ–‡æ¡£ (å·¥ä½œçº¿ç¨‹: {max_workers})...")
    start_time = time.time()

    # å¦‚æœä½¿ç”¨ UNOï¼Œé¢„å…ˆè¿æ¥æœåŠ¡
    if use_uno:
        manager = get_uno_manager()
        logger.info("ğŸ“¡ UNO æœåŠ¡å·²è¿æ¥")

    results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_file = {
            executor.submit(convert_document, file_path, use_uno): file_path
            for file_path in file_paths
        }

        # æ”¶é›†ç»“æœ
        for future in concurrent.futures.as_completed(future_to_file):
            file_path = future_to_file[future]
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                logger.error(f"è½¬æ¢å¤±è´¥: {file_path} - {str(e)}")

    total_time = time.time() - start_time
    logger.info(f"â±ï¸ å¹¶è¡Œè½¬æ¢å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
    return results


def batch_convert_parallel_with_executor(
    file_paths: list, executor: concurrent.futures.ThreadPoolExecutor, use_uno: bool = True
):
    """ä½¿ç”¨æä¾›çš„çº¿ç¨‹æ± æ‰§è¡Œå¹¶è¡Œè½¬æ¢ï¼ˆé¿å…é‡å¤åˆ›å»ºçº¿ç¨‹æ± ï¼‰"""
    if not HAS_UNO and use_uno:
        logger.warning("âš ï¸ UNO API ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼")
        use_uno = False

    logger.info(f"ğŸš€ ä½¿ç”¨é¢„åˆ›å»ºçš„çº¿ç¨‹æ± è½¬æ¢ {len(file_paths)} ä¸ªæ–‡æ¡£...")
    start_time = time.time()

    # å¦‚æœä½¿ç”¨ UNOï¼Œé¢„å…ˆè¿æ¥æœåŠ¡
    if use_uno:
        manager = get_uno_manager()
        logger.info("ğŸ“¡ UNO æœåŠ¡å·²è¿æ¥")

    results = []
    # æäº¤æ‰€æœ‰ä»»åŠ¡
    future_to_file = {
        executor.submit(convert_document, file_path, use_uno): file_path
        for file_path in file_paths
    }

    # æ”¶é›†ç»“æœ
    for future in concurrent.futures.as_completed(future_to_file):
        file_path = future_to_file[future]
        try:
            result = future.result()
            results.append(result)
        except Exception as e:
            logger.error(f"è½¬æ¢å¤±è´¥: {file_path} - {str(e)}")

    total_time = time.time() - start_time
    logger.info(f"â±ï¸ è½¬æ¢å®Œæˆï¼Œè€—æ—¶: {total_time:.2f}ç§’")
    return results


async def async_convert_document(file_path: str, use_uno: bool = True):
    """å¼‚æ­¥è½¬æ¢æ–‡æ¡£"""
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(None, convert_document, file_path, use_uno)


async def batch_convert_async(file_paths: list, use_uno: bool = True):
    """å¼‚æ­¥æ‰¹é‡è½¬æ¢æ–‡æ¡£"""
    if not HAS_UNO and use_uno:
        logger.warning("âš ï¸ UNO API ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼")
        use_uno = False

    logger.info(f"âš¡ å¼€å§‹å¼‚æ­¥è½¬æ¢ {len(file_paths)} ä¸ªæ–‡æ¡£...")
    start_time = time.time()

    # å¦‚æœä½¿ç”¨ UNOï¼Œé¢„å…ˆè¿æ¥æœåŠ¡
    if use_uno:
        manager = get_uno_manager()
        logger.info("ğŸ“¡ UNO æœåŠ¡å·²è¿æ¥")

    # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
    tasks = [async_convert_document(file_path, use_uno) for file_path in file_paths]

    # æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # å¤„ç†ç»“æœ
    successful_results = []
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            logger.error(f"è½¬æ¢å¤±è´¥: {file_paths[i]} - {str(result)}")
        else:
            successful_results.append(result)

    total_time = time.time() - start_time
    logger.info(f"â±ï¸ å¼‚æ­¥è½¬æ¢å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
    return successful_results


def performance_comparison(file_paths: list):
    """è¯¦ç»†æ€§èƒ½å¯¹æ¯”æµ‹è¯•"""
    logger.info("=" * 80)
    logger.info("ğŸ“Š å¼€å§‹è¯¦ç»†æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    logger.info("=" * 80)

    # è¿‡æ»¤å­˜åœ¨çš„æ–‡ä»¶
    existing_files = [f for f in file_paths if os.path.exists(f)]
    if not existing_files:
        logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æµ‹è¯•æ–‡ä»¶")
        return

    logger.info(f"ğŸ“ æµ‹è¯•æ–‡ä»¶æ•°é‡: {len(existing_files)}")
    for i, file_path in enumerate(existing_files, 1):
        file_size = os.path.getsize(file_path) / 1024  # KB
        logger.info(f"   {i}. {os.path.basename(file_path)} ({file_size:.1f} KB)")

    results = {}

    # 1. æµ‹è¯•ä¼ ç»Ÿæ–¹å¼ï¼ˆé¡ºåºï¼Œä¸ä½¿ç”¨UNOï¼‰
    logger.info("\n" + "=" * 60)
    logger.info("ğŸŒ 1ï¸âƒ£ ä¼ ç»Ÿæ–¹å¼ - é¡ºåºå¤„ç†ï¼ˆä¸ä½¿ç”¨UNOï¼‰")
    logger.info("=" * 60)
    start_time = time.time()
    try:
        batch_convert_sequential(existing_files, use_uno=False)
        sequential_time = time.time() - start_time
        results["sequential_no_uno"] = sequential_time
        logger.info(f"ğŸ“Š ä¼ ç»Ÿé¡ºåºæ–¹å¼æ€»è€—æ—¶: {sequential_time:.2f}ç§’")
        logger.info(f"ğŸ“Š å¹³å‡æ¯æ–‡ä»¶: {sequential_time/len(existing_files):.2f}ç§’")
    except Exception as e:
        logger.error(f"âŒ ä¼ ç»Ÿæ–¹å¼æµ‹è¯•å¤±è´¥: {str(e)}")
        results["sequential_no_uno"] = None

    # 2. æµ‹è¯•ä¼ ç»Ÿæ–¹å¼ï¼ˆå¹¶è¡Œï¼Œä¸ä½¿ç”¨UNOï¼‰
    logger.info("\n" + "=" * 60)
    logger.info("âš¡ 2ï¸âƒ£ ä¼ ç»Ÿæ–¹å¼ - å¹¶è¡Œå¤„ç†ï¼ˆä¸ä½¿ç”¨UNOï¼Œ4çº¿ç¨‹ï¼‰")
    logger.info("=" * 60)
    start_time = time.time()
    try:
        batch_convert_parallel(existing_files, max_workers=4, use_uno=False)
        parallel_no_uno_time = time.time() - start_time
        results["parallel_no_uno"] = parallel_no_uno_time
        logger.info(f"ğŸ“Š ä¼ ç»Ÿå¹¶è¡Œæ–¹å¼æ€»è€—æ—¶: {parallel_no_uno_time:.2f}ç§’")
        logger.info(f"ğŸ“Š å¹³å‡æ¯æ–‡ä»¶: {parallel_no_uno_time/len(existing_files):.2f}ç§’")
        if "sequential_no_uno" in results and results["sequential_no_uno"]:
            speedup = results["sequential_no_uno"] / parallel_no_uno_time
            logger.info(f"ğŸ“Š ç›¸æ¯”é¡ºåºæå‡: {speedup:.2f}x")
    except Exception as e:
        logger.error(f"âŒ ä¼ ç»Ÿå¹¶è¡Œæ–¹å¼æµ‹è¯•å¤±è´¥: {str(e)}")
        results["parallel_no_uno"] = None

    if HAS_UNO:
        # 3. æµ‹è¯•UNOæ–¹å¼ï¼ˆå¹¶è¡Œï¼‰
        logger.info("\n" + "=" * 60)
        logger.info("ğŸš€ 3ï¸âƒ£ UNO API - å¹¶è¡Œå¤„ç†ï¼ˆ4çº¿ç¨‹ï¼‰")
        logger.info("=" * 60)
        start_time = time.time()
        try:
            batch_convert_parallel(existing_files, max_workers=4, use_uno=True)
            uno_parallel_time = time.time() - start_time
            results["uno_parallel"] = uno_parallel_time
            logger.info(f"ğŸ“Š UNOå¹¶è¡Œæ–¹å¼æ€»è€—æ—¶: {uno_parallel_time:.2f}ç§’")
            logger.info(f"ğŸ“Š å¹³å‡æ¯æ–‡ä»¶: {uno_parallel_time/len(existing_files):.2f}ç§’")
        except Exception as e:
            logger.error(f"âŒ UNOå¹¶è¡Œæ–¹å¼æµ‹è¯•å¤±è´¥: {str(e)}")
            results["uno_parallel"] = None

        # 4. æµ‹è¯•UNOæ–¹å¼ï¼ˆé«˜å¹¶å‘ï¼‰
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ”¥ 4ï¸âƒ£ UNO API - é«˜å¹¶å‘å¤„ç†ï¼ˆ8çº¿ç¨‹ï¼‰")
        logger.info("=" * 60)
        start_time = time.time()
        try:
            batch_convert_parallel(existing_files, max_workers=8, use_uno=True)
            uno_high_parallel_time = time.time() - start_time
            results["uno_high_parallel"] = uno_high_parallel_time
            logger.info(f"ğŸ“Š UNOé«˜å¹¶å‘æ–¹å¼æ€»è€—æ—¶: {uno_high_parallel_time:.2f}ç§’")
            logger.info(f"ğŸ“Š å¹³å‡æ¯æ–‡ä»¶: {uno_high_parallel_time/len(existing_files):.2f}ç§’")
        except Exception as e:
            logger.error(f"âŒ UNOé«˜å¹¶å‘æ–¹å¼æµ‹è¯•å¤±è´¥: {str(e)}")
            results["uno_high_parallel"] = None
    else:
        logger.warning("âš ï¸ UNO API ä¸å¯ç”¨ï¼Œè·³è¿‡ UNO æ€§èƒ½æµ‹è¯•")

    # æ€§èƒ½æ€»ç»“
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ“ˆ æ€§èƒ½å¯¹æ¯”æ€»ç»“")
    logger.info("=" * 80)

    # æ„å»ºç»“æœè¡¨æ ¼
    methods = [
        ("sequential_no_uno", "ä¼ ç»Ÿé¡ºåºæ–¹å¼"),
        ("parallel_no_uno", "ä¼ ç»Ÿå¹¶è¡Œæ–¹å¼(4çº¿ç¨‹)"),
        ("uno_parallel", "UNOå¹¶è¡Œæ–¹å¼(4çº¿ç¨‹)"),
        ("uno_high_parallel", "UNOé«˜å¹¶å‘æ–¹å¼(8çº¿ç¨‹)"),
    ]

    logger.info(f"{'æ–¹æ³•':<25} {'æ€»æ—¶é—´(ç§’)':<12} {'å¹³å‡æ—¶é—´(ç§’)':<15} {'ç›¸å¯¹æ€§èƒ½':<12}")
    logger.info("-" * 70)

    baseline_time = None
    for key, name in methods:
        if key in results and results[key] is not None:
            total_time = results[key]
            avg_time = total_time / len(existing_files)

            if baseline_time is None:
                baseline_time = total_time
                relative = "1.00x (åŸºå‡†)"
            else:
                ratio = baseline_time / total_time
                relative = f"{ratio:.2f}x"

            logger.info(
                f"{name:<25} {total_time:<12.2f} {avg_time:<15.2f} {relative:<12}"
            )

    # æ€§èƒ½åˆ†æ
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ” æ€§èƒ½åˆ†æ")
    logger.info("=" * 80)

    if results.get("parallel_no_uno") and results.get("uno_parallel"):
        ratio = results["uno_parallel"] / results["parallel_no_uno"]
        if ratio > 1:
            logger.info(f"âœ… ä¼ ç»Ÿå¹¶è¡Œæ–¹å¼æ¯”UNOå¹¶è¡Œæ–¹å¼å¿« {ratio:.2f}x")
            logger.info("ğŸ’¡ å»ºè®®åœ¨è¿½æ±‚æ€§èƒ½æ—¶ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼ï¼ˆuse_uno=Falseï¼‰")
        else:
            logger.info(f"âœ… UNOå¹¶è¡Œæ–¹å¼æ¯”ä¼ ç»Ÿå¹¶è¡Œæ–¹å¼å¿« {1/ratio:.2f}x")
            logger.info("ğŸ’¡ UNOæ–¹å¼åœ¨è¿™ç§æƒ…å†µä¸‹æ€§èƒ½æ›´ä¼˜")

    logger.info("\nğŸ§  æ€§èƒ½å·®å¼‚åŸå› åˆ†æ:")
    logger.info("  ğŸŒ UNOæ…¢çš„åŸå› :")
    logger.info("     â€¢ LibreOfficeæ˜¯é‡é‡çº§åº”ç”¨ï¼Œå¯åŠ¨å’Œè¿è¡Œå¼€é”€å¤§")
    logger.info("     â€¢ éœ€è¦è¿›ç¨‹é—´é€šä¿¡(IPC)ï¼Œæœ‰ç½‘ç»œå»¶è¿Ÿ")
    logger.info("     â€¢ éœ€è¦å®Œæ•´åŠ è½½æ–‡æ¡£åˆ°å†…å­˜")
    logger.info("     â€¢ æœåŠ¡è¿æ¥å’Œç®¡ç†æœ‰é¢å¤–å¼€é”€")
    logger.info("  âš¡ ä¼ ç»Ÿæ–¹å¼å¿«çš„åŸå› :")
    logger.info("     â€¢ ç›´æ¥è°ƒç”¨sofficeå‘½ä»¤ï¼Œæ— ä¸­é—´å±‚")
    logger.info("     â€¢ æ¯æ¬¡è°ƒç”¨éƒ½æ˜¯ç‹¬ç«‹è¿›ç¨‹ï¼Œæ— çŠ¶æ€ç®¡ç†")
    logger.info("     â€¢ å‘½ä»¤è¡Œè°ƒç”¨å¼€é”€ç›¸å¯¹è¾ƒå°")
    logger.info("  ğŸ¤” ä½•æ—¶ä½¿ç”¨UNO:")
    logger.info("     â€¢ éœ€è¦ç²¾ç»†æ§åˆ¶æ–‡æ¡£è½¬æ¢è¿‡ç¨‹")
    logger.info("     â€¢ éœ€è¦å¤æ‚çš„æ–‡æ¡£æ“ä½œå’Œæ ¼å¼åŒ–")
    logger.info("     â€¢ åœ¨é•¿æ—¶é—´è¿è¡Œçš„æœåŠ¡ä¸­ï¼Œå¯ä»¥å¤ç”¨è¿æ¥")

    logger.info("\n" + "=" * 80)
    logger.info("âœ… æ€§èƒ½å¯¹æ¯”æµ‹è¯•å®Œæˆ")
    logger.info("=" * 80)


def convert_document_with_manager_info(file_path: str, use_uno: bool = True):
    """è½¬æ¢å•ä¸ªæ–‡æ¡£å¹¶æ˜¾ç¤ºUNOç®¡ç†å™¨ä¿¡æ¯"""
    import threading

    thread_id = threading.current_thread().ident

    start_time = time.time()

    try:
        if file_path.lower().endswith(".doc"):
            parser = DocParser(str(file_path), use_uno=use_uno)
        elif file_path.lower().endswith(".docx"):
            parser = DocxParser(str(file_path), use_uno=use_uno)
        elif file_path.lower().endswith(".ppt"):
            parser = PPtParser(str(file_path), use_uno=use_uno)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path}")

        # å¦‚æœä½¿ç”¨UNOï¼Œæ˜¾ç¤ºç®¡ç†å™¨ä¿¡æ¯
        if use_uno:
            manager = get_uno_manager()
            logger.info(f"ğŸ¯ [çº¿ç¨‹{thread_id}] ä½¿ç”¨UnoManager (ç«¯å£: {manager.port})")

        result = parser.parse(str(file_path))
        elapsed_time = time.time() - start_time
        logger.info(
            f"âœ… [çº¿ç¨‹{thread_id}] è½¬æ¢æˆåŠŸ: {os.path.basename(file_path)} (è€—æ—¶: {elapsed_time:.2f}ç§’)"
        )
        
        # é‡Šæ”¾UNOç®¡ç†å™¨å›åˆ°æ± ä¸­
        if use_uno:
            release_uno_manager()
            logger.debug(f"â™»ï¸ [çº¿ç¨‹{thread_id}] å·²é‡Šæ”¾UnoManager")
        
        return result

    except Exception as e:
        elapsed_time = time.time() - start_time
        logger.error(
            f"âŒ [çº¿ç¨‹{thread_id}] è½¬æ¢å¤±è´¥: {os.path.basename(file_path)} - {str(e)} (è€—æ—¶: {elapsed_time:.2f}ç§’)"
        )
        # ç¡®ä¿é‡Šæ”¾ç®¡ç†å™¨
        if use_uno:
            release_uno_manager()
        raise


def batch_convert_with_manager_info(
    file_paths: list, max_workers: int = 4, use_uno: bool = True
):
    """å¹¶è¡Œè½¬æ¢å¤šä¸ªæ–‡æ¡£å¹¶æ˜¾ç¤ºç®¡ç†å™¨ä¿¡æ¯"""
    if not HAS_UNO and use_uno:
        logger.warning("âš ï¸ UNO API ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼")
        use_uno = False

    logger.info(f"ğŸš€ å¼€å§‹å¹¶è¡Œè½¬æ¢ {len(file_paths)} ä¸ªæ–‡æ¡£ (å·¥ä½œçº¿ç¨‹: {max_workers})...")
    start_time = time.time()

    results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_file = {
            executor.submit(
                convert_document_with_manager_info, file_path, use_uno
            ): file_path
            for file_path in file_paths
        }

        # æ”¶é›†ç»“æœ
        for future in concurrent.futures.as_completed(future_to_file):
            file_path = future_to_file[future]
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                logger.error(f"è½¬æ¢å¤±è´¥: {file_path} - {str(e)}")

    total_time = time.time() - start_time
    logger.info(f"â±ï¸ å¹¶è¡Œè½¬æ¢å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
    return results


def batch_convert_with_manager_info_with_executor(
    file_paths: list, executor: concurrent.futures.ThreadPoolExecutor, use_uno: bool = True
):
    """ä½¿ç”¨æä¾›çš„çº¿ç¨‹æ± æ‰§è¡Œå¹¶è¡Œè½¬æ¢å¹¶æ˜¾ç¤ºç®¡ç†å™¨ä¿¡æ¯ï¼ˆé¿å…é‡å¤åˆ›å»ºçº¿ç¨‹æ± ï¼‰"""
    if not HAS_UNO and use_uno:
        logger.warning("âš ï¸ UNO API ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼")
        use_uno = False

    logger.info(f"ğŸš€ ä½¿ç”¨é¢„åˆ›å»ºçš„çº¿ç¨‹æ± è½¬æ¢ {len(file_paths)} ä¸ªæ–‡æ¡£...")
    start_time = time.time()

    results = []
    # æäº¤æ‰€æœ‰ä»»åŠ¡
    future_to_file = {
        executor.submit(
            convert_document_with_manager_info, file_path, use_uno
        ): file_path
        for file_path in file_paths
    }

    # æ”¶é›†ç»“æœ
    for future in concurrent.futures.as_completed(future_to_file):
        file_path = future_to_file[future]
        try:
            result = future.result()
            results.append(result)
        except Exception as e:
            logger.error(f"è½¬æ¢å¤±è´¥: {file_path} - {str(e)}")

    total_time = time.time() - start_time
    logger.info(f"â±ï¸ è½¬æ¢å®Œæˆï¼Œè€—æ—¶: {total_time:.2f}ç§’")
    return results


def traditional_stress_test(base_files: list, repeat_count: int = 3):
    """ä¼ ç»ŸLibreOfficeæ–¹å¼å‹åŠ›æµ‹è¯•"""
    logger.info("=" * 100)
    logger.info("âš¡ ä¼ ç»ŸLibreOfficeæ–¹å¼å¹¶è¡Œæ€§èƒ½å‹åŠ›æµ‹è¯•")
    logger.info("=" * 100)

    # æ‰©å……æ–‡ä»¶åˆ—è¡¨ - é‡å¤è°ƒç”¨ç›¸åŒæ–‡æ¡£
    expanded_files = []
    for i in range(repeat_count):
        for file_path in base_files:
            if os.path.exists(file_path):
                expanded_files.append(file_path)

    if not expanded_files:
        logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æµ‹è¯•æ–‡ä»¶")
        return

    logger.info(f"ğŸ“ å‹æµ‹é…ç½®:")
    logger.info(f"   åŸºç¡€æ–‡ä»¶æ•°: {len(base_files)}")
    logger.info(f"   é‡å¤æ¬¡æ•°: {repeat_count}")
    logger.info(f"   æ€»æ–‡ä»¶æ•°: {len(expanded_files)}")

    # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
    unique_files = list(set(expanded_files))
    total_size = 0
    for file_path in unique_files:
        file_size = os.path.getsize(file_path) / 1024  # KB
        total_size += file_size
        count = expanded_files.count(file_path)
        logger.info(f"   ğŸ“„ {os.path.basename(file_path)}: {file_size:.1f}KB Ã— {count}æ¬¡")

    logger.info(f"   ğŸ“Š æ€»æ•°æ®é‡: {total_size * repeat_count:.1f}KB")

    logger.info(f"ğŸš€ ä¼ ç»ŸLibreOfficeå¹¶è¡Œæ¶æ„:")
    logger.info(f"   ğŸ¯ æ¯ä¸ªçº¿ç¨‹ä½¿ç”¨ç‹¬ç«‹çš„sofficeè¿›ç¨‹")
    logger.info(f"   âš¡ ç›´æ¥è°ƒç”¨å‘½ä»¤è¡Œï¼Œæ— ä¸­é—´å±‚")
    logger.info(f"   ğŸ”„ æ— çŠ¶æ€ç®¡ç†ï¼Œè¿›ç¨‹ç‹¬ç«‹è¿è¡Œ")

    # æµ‹è¯•ä¸åŒçº¿ç¨‹æ•°çš„ä¼ ç»Ÿæ–¹å¼æ€§èƒ½
    thread_configs = [1, 4, 8, 12]
    results = {}
    baseline_time = None
    
    # æå‰åˆ›å»ºæ‰€æœ‰çº¿ç¨‹æ± 
    logger.info(f"\nğŸ”§ é¢„åˆ›å»ºæ‰€æœ‰çº¿ç¨‹æ± ...")
    executors = {}
    for workers in thread_configs:
        executors[workers] = concurrent.futures.ThreadPoolExecutor(max_workers=workers)
        logger.info(f"   âœ… åˆ›å»º{workers}çº¿ç¨‹æ± å®Œæˆ")
        # é¢„çƒ­çº¿ç¨‹æ± 
        warmup_thread_pool(executors[workers])
    
    logger.info(f"ğŸ‰ æ‰€æœ‰çº¿ç¨‹æ± å‡†å¤‡å°±ç»ªï¼Œå¼€å§‹æµ‹è¯•...\n")

    try:
        for max_workers in thread_configs:
            logger.info(f"\n{'='*80}")
            logger.info(f"âš¡ æµ‹è¯•ä¼ ç»Ÿæ–¹å¼ - {max_workers} çº¿ç¨‹å¹¶è¡Œå¤„ç†")
            logger.info(f"{'='*80}")

            start_time = time.time()

            try:
                # ä½¿ç”¨é¢„åˆ›å»ºçš„çº¿ç¨‹æ± è¿›è¡Œå¹¶è¡Œè½¬æ¢
                converted_results = batch_convert_parallel_with_executor(
                    expanded_files, executor=executors[max_workers], use_uno=False
                )

                total_time = time.time() - start_time
                successful_count = len([r for r in converted_results if r is not None])

                # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
                avg_time_per_file = total_time / len(expanded_files)
                throughput = len(expanded_files) / total_time  # æ–‡ä»¶/ç§’

                # ç†è®ºæœ€ä¼˜æ—¶é—´ï¼ˆåŸºäºå•çº¿ç¨‹æ—¶é—´ï¼‰
                if max_workers == 1:
                    baseline_time = total_time
                    efficiency = 1.0
                else:
                    theoretical_time = baseline_time / max_workers
                    efficiency = theoretical_time / total_time if total_time > 0 else 0

                results[max_workers] = {
                    "total_time": total_time,
                    "successful_count": successful_count,
                    "avg_time_per_file": avg_time_per_file,
                    "throughput": throughput,
                    "efficiency": efficiency,
                    "files_processed": len(expanded_files),
                }

                logger.info(f"ğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
                logger.info(f"   æ€»è€—æ—¶: {total_time:.2f}ç§’")
                logger.info(f"   æˆåŠŸè½¬æ¢: {successful_count}/{len(expanded_files)}")
                logger.info(f"   å¹³å‡æ—¶é—´: {avg_time_per_file:.2f}ç§’/æ–‡ä»¶")
                logger.info(f"   ååé‡: {throughput:.2f}æ–‡ä»¶/ç§’")
                if max_workers > 1:
                    logger.info(f"   å¹¶è¡Œæ•ˆç‡: {efficiency:.2f}x (ç†æƒ³: {max_workers}x)")
                    efficiency_percentage = (efficiency / max_workers) * 100
                    logger.info(f"   æ•ˆç‡ç™¾åˆ†æ¯”: {efficiency_percentage:.1f}%")

            except Exception as e:
                logger.error(f"âŒ {max_workers}çº¿ç¨‹æµ‹è¯•å¤±è´¥: {str(e)}")
                results[max_workers] = {
                    "error": str(e),
                    "total_time": 0,
                    "successful_count": 0,
                }
    finally:
        # æ¸…ç†æ‰€æœ‰çº¿ç¨‹æ± 
        logger.info(f"\nğŸ§¹ æ¸…ç†çº¿ç¨‹æ± ...")
        for workers, executor in executors.items():
            executor.shutdown(wait=True)
            logger.info(f"   âœ… {workers}çº¿ç¨‹æ± å·²å…³é—­")
        logger.info(f"ğŸ‰ æ‰€æœ‰çº¿ç¨‹æ± å·²æ¸…ç†")

    # ç»¼åˆæ€§èƒ½åˆ†æ
    logger.info(f"\n{'='*100}")
    logger.info("ğŸ“ˆ ä¼ ç»ŸLibreOfficeå‹åŠ›æµ‹è¯•ç»¼åˆåˆ†æ")
    logger.info(f"{'='*100}")

    # æ€§èƒ½å¯¹æ¯”è¡¨æ ¼
    logger.info(
        f"{'çº¿ç¨‹æ•°':<8} {'æ€»æ—¶é—´(ç§’)':<12} {'æˆåŠŸç‡':<10} {'ååé‡(æ–‡ä»¶/ç§’)':<18} {'å¹¶è¡Œæ•ˆç‡':<12} {'æ•ˆç‡ç™¾åˆ†æ¯”':<12}"
    )
    logger.info("-" * 85)

    baseline_throughput = None
    for max_workers in sorted(results.keys()):
        result = results[max_workers]
        if "error" in result:
            logger.info(
                f"{max_workers:<8} {'ERROR':<12} {'N/A':<10} {'N/A':<18} {'N/A':<12} {'N/A':<12}"
            )
            continue

        total_time = result["total_time"]
        success_rate = f"{result['successful_count']}/{result['files_processed']}"
        throughput = result["throughput"]
        efficiency = result.get("efficiency", 1.0)
        efficiency_percentage = (
            (efficiency / max_workers) * 100 if max_workers > 1 else 100
        )

        if baseline_throughput is None:
            baseline_throughput = throughput

        logger.info(
            f"{max_workers:<8} {total_time:<12.2f} {success_rate:<10} "
            f"{throughput:<18.2f} {efficiency:<12.2f} {efficiency_percentage:<12.1f}%"
        )

    # æ€§èƒ½åˆ†æå’Œå»ºè®®
    logger.info(f"\nğŸ” ä¼ ç»Ÿæ–¹å¼æ€§èƒ½åˆ†æ:")

    # æ‰¾å‡ºæœ€ä½³é…ç½®
    valid_results = {
        k: v
        for k, v in results.items()
        if "error" not in v and v["successful_count"] > 0
    }
    if valid_results:
        best_throughput = max(v["throughput"] for v in valid_results.values())
        best_config = [
            k for k, v in valid_results.items() if v["throughput"] == best_throughput
        ][0]

        logger.info(f"   ğŸ¯ æœ€ä½³æ€§èƒ½é…ç½®: {best_config}çº¿ç¨‹ (ååé‡: {best_throughput:.2f}æ–‡ä»¶/ç§’)")

        # åˆ†ææ‰©å±•æ€§
        if len(valid_results) >= 3:
            efficiency_4 = valid_results.get(4, {}).get("efficiency", 0)
            efficiency_8 = valid_results.get(8, {}).get("efficiency", 0)
            efficiency_12 = valid_results.get(12, {}).get("efficiency", 0)

            logger.info(f"   ğŸ“Š æ‰©å±•æ€§åˆ†æ:")
            if efficiency_4 > 0:
                logger.info(
                    f"      4çº¿ç¨‹æ•ˆç‡: {efficiency_4:.2f}x ({(efficiency_4/4)*100:.1f}%)"
                )
            if efficiency_8 > 0:
                logger.info(
                    f"      8çº¿ç¨‹æ•ˆç‡: {efficiency_8:.2f}x ({(efficiency_8/8)*100:.1f}%)"
                )
            if efficiency_12 > 0:
                logger.info(
                    f"      12çº¿ç¨‹æ•ˆç‡: {efficiency_12:.2f}x ({(efficiency_12/12)*100:.1f}%)"
                )

            # åˆ†æLibreOfficeå‡å¤šçº¿ç¨‹é—®é¢˜
            if efficiency_4 < 2.0:
                logger.warning("   âš ï¸  ä¼ ç»Ÿæ–¹å¼å­˜åœ¨æ˜æ˜¾çš„å¹¶è¡Œç“¶é¢ˆ")
                if efficiency_4 < 1.5:
                    logger.warning("      ğŸ”´ ä¸¥é‡ç“¶é¢ˆï¼šå¯èƒ½æ˜¯LibreOfficeå…¨å±€é”")
                else:
                    logger.warning("      ğŸŸ¡ ä¸­ç­‰ç“¶é¢ˆï¼šå¯èƒ½æ˜¯I/Oæˆ–èµ„æºç«äº‰")
            elif efficiency_8 < 4.0:
                logger.warning("   âš ï¸  8çº¿ç¨‹ä»¥ä¸Šå­˜åœ¨æ€§èƒ½é€’å‡")
                logger.info("      ğŸŸ¡ å»ºè®®ä½¿ç”¨4çº¿ç¨‹ä»¥å†…è·å¾—æœ€ä½³æ€§ä»·æ¯”")
            else:
                logger.info("   âœ… ä¼ ç»Ÿæ–¹å¼å¹¶è¡Œæ€§èƒ½è‰¯å¥½")

            # åˆ¤æ–­æœ€ä½³çº¿ç¨‹æ•°
            if efficiency_4 > efficiency_8 and efficiency_4 > efficiency_12:
                logger.info("   ğŸ’¡ 4çº¿ç¨‹æ˜¯æœ€ä½³é€‰æ‹©")
            elif efficiency_8 > efficiency_12:
                logger.info("   ğŸ’¡ 8çº¿ç¨‹æ˜¯åˆç†ä¸Šé™")

    logger.info(f"\nğŸ’¡ ä¼ ç»Ÿæ–¹å¼ä¼˜åŒ–å»ºè®®:")
    if valid_results:
        best_efficiency_ratio = max(
            (v.get("efficiency", 0) / k) for k, v in valid_results.items() if k > 1
        )

        if best_efficiency_ratio > 0.7:
            logger.info("   âœ… ä¼ ç»Ÿæ–¹å¼å¹¶è¡Œæ•ˆç‡è‰¯å¥½")
        elif best_efficiency_ratio > 0.5:
            logger.info("   ğŸŸ¡ ä¼ ç»Ÿæ–¹å¼å¹¶è¡Œæ•ˆç‡ä¸­ç­‰")
        else:
            logger.info("   ğŸ”´ ä¼ ç»Ÿæ–¹å¼å¹¶è¡Œæ•ˆç‡è¾ƒä½")

        logger.info(f"   ğŸ¯ æ¨èé…ç½®: {best_config}çº¿ç¨‹ ç”¨äºä¼ ç»ŸLibreOfficeå¤„ç†")
        logger.info(f"   ğŸ“Š é¢„æœŸæ€§èƒ½: {best_throughput:.2f}æ–‡ä»¶/ç§’")

        # ä¸ç†æƒ³å¹¶è¡Œçš„å¯¹æ¯”
        ideal_speedup = best_config
        actual_speedup = valid_results[best_config]["efficiency"]
        parallel_loss = (1 - actual_speedup / ideal_speedup) * 100
        logger.info(
            f"   ğŸ“‰ å¹¶è¡ŒæŸå¤±: {parallel_loss:.1f}% (ç†æƒ³{ideal_speedup}x vs å®é™…{actual_speedup:.2f}x)"
        )

    return results


def uno_stress_test(base_files: list, repeat_count: int = 3):
    """UNO API å‹åŠ›æµ‹è¯•"""
    logger.info("=" * 100)
    logger.info("ğŸ”¥ UNO API å¹¶è¡Œæ€§èƒ½å‹åŠ›æµ‹è¯• - å¤šUNOæœåŠ¡ç‰ˆæœ¬")
    logger.info("=" * 100)

    # æ¸…ç†ä¹‹å‰çš„UNOç®¡ç†å™¨
    if cleanup_uno_managers:
        cleanup_uno_managers()
        logger.info("ğŸ§¹ å·²æ¸…ç†ä¹‹å‰çš„UNOç®¡ç†å™¨")
        time.sleep(2)  # ç­‰å¾…æœåŠ¡å®Œå…¨å…³é—­

    # æ‰©å……æ–‡ä»¶åˆ—è¡¨ - é‡å¤è°ƒç”¨ç›¸åŒæ–‡æ¡£
    expanded_files = []
    for i in range(repeat_count):
        for file_path in base_files:
            if os.path.exists(file_path):
                expanded_files.append(file_path)

    if not expanded_files:
        logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æµ‹è¯•æ–‡ä»¶")
        return

    logger.info(f"ğŸ“ å‹æµ‹é…ç½®:")
    logger.info(f"   åŸºç¡€æ–‡ä»¶æ•°: {len(base_files)}")
    logger.info(f"   é‡å¤æ¬¡æ•°: {repeat_count}")
    logger.info(f"   æ€»æ–‡ä»¶æ•°: {len(expanded_files)}")

    # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
    unique_files = list(set(expanded_files))
    total_size = 0
    for file_path in unique_files:
        file_size = os.path.getsize(file_path) / 1024  # KB
        total_size += file_size
        count = expanded_files.count(file_path)
        logger.info(f"   ğŸ“„ {os.path.basename(file_path)}: {file_size:.1f}KB Ã— {count}æ¬¡")

    logger.info(f"   ğŸ“Š æ€»æ•°æ®é‡: {total_size * repeat_count:.1f}KB")

    if not HAS_UNO:
        logger.error("âŒ UNO API ä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œå‹æµ‹")
        return

    logger.info(f"ğŸŠ å¤šUNOæœåŠ¡å¹¶è¡Œæ¶æ„:")
    logger.info(f"   ğŸ¯ æ¯ä¸ªçº¿ç¨‹å°†ä½¿ç”¨ç‹¬ç«‹çš„UNOæœåŠ¡å®ä¾‹")
    logger.info(f"   ğŸ”Œ UNOæœåŠ¡ç«¯å£èŒƒå›´: 2002-2009")
    logger.info(f"   ğŸš€ æ”¯æŒçœŸæ­£çš„å¹¶è¡Œå¤„ç†")
    
    # æ˜¾ç¤ºè¿æ¥æ± é…ç½®ä¿¡æ¯
    pool = get_uno_pool()
    logger.info(f"   ğŸ“Š è¿æ¥æ± æœ€å¤§ç®¡ç†å™¨æ•°: {pool.max_managers}")
    logger.info(f"   â™»ï¸  æ”¯æŒç®¡ç†å™¨å¤ç”¨ï¼Œæé«˜æ€§èƒ½")

    # æµ‹è¯•ä¸åŒçº¿ç¨‹æ•°çš„UNOæ€§èƒ½
    thread_configs = [1, 4, 8, 12]
    results = {}
    baseline_time = None
    
    # é¢„åˆ›å»ºæ‰€æœ‰UNOç®¡ç†å™¨
    max_uno_managers = max(thread_configs)
    logger.info(f"\nğŸ”§ é¢„åˆ›å»º {max_uno_managers} ä¸ªUNOç®¡ç†å™¨...")
    start_time = time.time()
    created_count = pre_create_uno_managers(max_uno_managers)
    creation_time = time.time() - start_time
    logger.info(f"âœ… æˆåŠŸåˆ›å»º {created_count} ä¸ªUNOç®¡ç†å™¨ï¼Œè€—æ—¶ {creation_time:.2f}ç§’")
    
    # é¢„çƒ­UNOç®¡ç†å™¨
    logger.info(f"\nâš¡ é¢„çƒ­æ‰€æœ‰UNOç®¡ç†å™¨...")
    start_time = time.time()
    warmup_uno_managers()
    warmup_time = time.time() - start_time
    logger.info(f"âœ… é¢„çƒ­å®Œæˆï¼Œè€—æ—¶ {warmup_time:.2f}ç§’")
    
    # æå‰åˆ›å»ºæ‰€æœ‰çº¿ç¨‹æ± 
    logger.info(f"\nğŸ”§ é¢„åˆ›å»ºæ‰€æœ‰çº¿ç¨‹æ± ...")
    executors = {}
    for workers in thread_configs:
        executors[workers] = concurrent.futures.ThreadPoolExecutor(max_workers=workers)
        logger.info(f"   âœ… åˆ›å»º{workers}çº¿ç¨‹æ± å®Œæˆ")
        # é¢„çƒ­çº¿ç¨‹æ± 
        warmup_thread_pool(executors[workers])
    
    logger.info(f"ğŸ‰ æ‰€æœ‰èµ„æºå‡†å¤‡å°±ç»ªï¼Œå¼€å§‹æµ‹è¯•...\n")

    try:
        for max_workers in thread_configs:
            logger.info(f"\n{'='*80}")
            logger.info(f"ğŸš€ æµ‹è¯• UNO API - {max_workers} çº¿ç¨‹å¹¶è¡Œå¤„ç†")
            logger.info(f"{'='*80}")

            start_time = time.time()

            try:
                # ä½¿ç”¨é¢„åˆ›å»ºçš„çº¿ç¨‹æ± è¿›è¡ŒUNOå¹¶è¡Œè½¬æ¢ï¼Œæ˜¾ç¤ºç®¡ç†å™¨ä¿¡æ¯
                converted_results = batch_convert_with_manager_info_with_executor(
                    expanded_files, executor=executors[max_workers], use_uno=True
                )

                total_time = time.time() - start_time
                successful_count = len([r for r in converted_results if r is not None])

                # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
                avg_time_per_file = total_time / len(expanded_files)
                throughput = len(expanded_files) / total_time  # æ–‡ä»¶/ç§’

                # ç†è®ºæœ€ä¼˜æ—¶é—´ï¼ˆåŸºäºå•çº¿ç¨‹æ—¶é—´ï¼‰
                if max_workers == 1:
                    baseline_time = total_time
                    efficiency = 1.0
                else:
                    theoretical_time = baseline_time / max_workers
                    efficiency = theoretical_time / total_time if total_time > 0 else 0

                results[max_workers] = {
                    "total_time": total_time,
                    "successful_count": successful_count,
                    "avg_time_per_file": avg_time_per_file,
                    "throughput": throughput,
                    "efficiency": efficiency,
                    "files_processed": len(expanded_files),
                }

                logger.info(f"ğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
                logger.info(f"   æ€»è€—æ—¶: {total_time:.2f}ç§’")
                logger.info(f"   æˆåŠŸè½¬æ¢: {successful_count}/{len(expanded_files)}")
                logger.info(f"   å¹³å‡æ—¶é—´: {avg_time_per_file:.2f}ç§’/æ–‡ä»¶")
                logger.info(f"   ååé‡: {throughput:.2f}æ–‡ä»¶/ç§’")
                if max_workers > 1:
                    logger.info(f"   å¹¶è¡Œæ•ˆç‡: {efficiency:.2f}x (ç†æƒ³: {max_workers}x)")
                    efficiency_percentage = (efficiency / max_workers) * 100
                    logger.info(f"   æ•ˆç‡ç™¾åˆ†æ¯”: {efficiency_percentage:.1f}%")

            except Exception as e:
                logger.error(f"âŒ {max_workers}çº¿ç¨‹æµ‹è¯•å¤±è´¥: {str(e)}")
                results[max_workers] = {
                    "error": str(e),
                    "total_time": 0,
                    "successful_count": 0,
                }
    finally:
        # æ¸…ç†æ‰€æœ‰çº¿ç¨‹æ± 
        logger.info(f"\nğŸ§¹ æ¸…ç†çº¿ç¨‹æ± ...")
        for workers, executor in executors.items():
            executor.shutdown(wait=True)
            logger.info(f"   âœ… {workers}çº¿ç¨‹æ± å·²å…³é—­")
        logger.info(f"ğŸ‰ æ‰€æœ‰çº¿ç¨‹æ± å·²æ¸…ç†")

    # ç»¼åˆæ€§èƒ½åˆ†æ
    logger.info(f"\n{'='*100}")
    logger.info("ğŸ“ˆ UNO API å‹åŠ›æµ‹è¯•ç»¼åˆåˆ†æ")
    logger.info(f"{'='*100}")

    # æ€§èƒ½å¯¹æ¯”è¡¨æ ¼
    logger.info(
        f"{'çº¿ç¨‹æ•°':<8} {'æ€»æ—¶é—´(ç§’)':<12} {'æˆåŠŸç‡':<10} {'ååé‡(æ–‡ä»¶/ç§’)':<18} {'å¹¶è¡Œæ•ˆç‡':<12} {'æ•ˆç‡ç™¾åˆ†æ¯”':<12}"
    )
    logger.info("-" * 85)

    baseline_throughput = None
    for max_workers in sorted(results.keys()):
        result = results[max_workers]
        if "error" in result:
            logger.info(
                f"{max_workers:<8} {'ERROR':<12} {'N/A':<10} {'N/A':<18} {'N/A':<12} {'N/A':<12}"
            )
            continue

        total_time = result["total_time"]
        success_rate = f"{result['successful_count']}/{result['files_processed']}"
        throughput = result["throughput"]
        efficiency = result.get("efficiency", 1.0)
        efficiency_percentage = (
            (efficiency / max_workers) * 100 if max_workers > 1 else 100
        )

        if baseline_throughput is None:
            baseline_throughput = throughput

        logger.info(
            f"{max_workers:<8} {total_time:<12.2f} {success_rate:<10} "
            f"{throughput:<18.2f} {efficiency:<12.2f} {efficiency_percentage:<12.1f}%"
        )

    # æ€§èƒ½åˆ†æå’Œå»ºè®®
    logger.info(f"\nğŸ” æ€§èƒ½åˆ†æ:")

    # æ‰¾å‡ºæœ€ä½³é…ç½®
    valid_results = {
        k: v
        for k, v in results.items()
        if "error" not in v and v["successful_count"] > 0
    }
    if valid_results:
        best_throughput = max(v["throughput"] for v in valid_results.values())
        best_config = [
            k for k, v in valid_results.items() if v["throughput"] == best_throughput
        ][0]

        logger.info(f"   ğŸ¯ æœ€ä½³æ€§èƒ½é…ç½®: {best_config}çº¿ç¨‹ (ååé‡: {best_throughput:.2f}æ–‡ä»¶/ç§’)")

        # åˆ†ææ‰©å±•æ€§
        if len(valid_results) >= 3:
            efficiency_4 = valid_results.get(4, {}).get("efficiency", 0)
            efficiency_8 = valid_results.get(8, {}).get("efficiency", 0)
            efficiency_12 = valid_results.get(12, {}).get("efficiency", 0)

            logger.info(f"   ğŸ“Š æ‰©å±•æ€§åˆ†æ:")
            if efficiency_4 > 0:
                logger.info(
                    f"      4çº¿ç¨‹æ•ˆç‡: {efficiency_4:.2f}x ({(efficiency_4/4)*100:.1f}%)"
                )
            if efficiency_8 > 0:
                logger.info(
                    f"      8çº¿ç¨‹æ•ˆç‡: {efficiency_8:.2f}x ({(efficiency_8/8)*100:.1f}%)"
                )
            if efficiency_12 > 0:
                logger.info(
                    f"      12çº¿ç¨‹æ•ˆç‡: {efficiency_12:.2f}x ({(efficiency_12/12)*100:.1f}%)"
                )

            # åˆ¤æ–­UNOçš„å¹¶è¡Œç“¶é¢ˆ
            if efficiency_8 < 4.0:
                logger.warning("   âš ï¸  UNO APIå­˜åœ¨æ˜æ˜¾çš„å¹¶è¡Œç“¶é¢ˆ")
                if efficiency_4 < 2.0:
                    logger.warning("      ğŸ”´ ä¸¥é‡ç“¶é¢ˆï¼šå¯èƒ½æ˜¯å…¨å±€é”æˆ–èµ„æºç«äº‰")
                else:
                    logger.warning("      ğŸŸ¡ ä¸­ç­‰ç“¶é¢ˆï¼šå»ºè®®ä½¿ç”¨4çº¿ç¨‹ä»¥å†…")
            else:
                logger.info("   âœ… UNO APIå¹¶è¡Œæ€§èƒ½è‰¯å¥½")

    # ä¸ä¼ ç»Ÿæ–¹å¼å¯¹æ¯”æç¤º
    logger.info(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
    if valid_results:
        best_efficiency_ratio = max(
            (v.get("efficiency", 0) / k) for k, v in valid_results.items() if k > 1
        )

        if best_efficiency_ratio < 0.3:
            logger.info("   ğŸ“ UNO APIå¹¶è¡Œæ•ˆç‡è¾ƒä½ï¼Œè€ƒè™‘ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼")
        elif best_efficiency_ratio < 0.6:
            logger.info("   ğŸ“ UNO APIæœ‰ä¸€å®šå¹¶è¡Œèƒ½åŠ›ï¼Œä½†ä¸å¦‚ä¼ ç»Ÿæ–¹å¼")
        else:
            logger.info("   ğŸ“ UNO APIå¹¶è¡Œæ€§èƒ½å¯æ¥å—")

        logger.info(f"   ğŸ¯ æ¨èé…ç½®: {best_config}çº¿ç¨‹ ç”¨äºUNO APIå¹¶è¡Œå¤„ç†")
        logger.info(f"   ğŸ“Š é¢„æœŸæ€§èƒ½: {best_throughput:.2f}æ–‡ä»¶/ç§’")

    # æ¸…ç†UNOç®¡ç†å™¨
    if cleanup_uno_managers:
        cleanup_uno_managers()
        logger.info("ğŸ§¹ å‹æµ‹å®Œæˆï¼Œå·²æ¸…ç†æ‰€æœ‰UNOç®¡ç†å™¨")

    return results


def comprehensive_stress_test(base_files: list, repeat_count: int = 3):
    """ç»¼åˆå‹åŠ›æµ‹è¯• - ä¼ ç»Ÿæ–¹å¼ vs UNOæ–¹å¼"""
    logger.info("=" * 120)
    logger.info("ğŸ† LibreOffice ç»¼åˆæ€§èƒ½å‹åŠ›æµ‹è¯• - ä¼ ç»Ÿæ–¹å¼ vs UNOæ–¹å¼")
    logger.info("=" * 120)

    # 1. ä¼ ç»Ÿæ–¹å¼å‹åŠ›æµ‹è¯•
    logger.info("\nğŸš€ ç¬¬ä¸€é˜¶æ®µï¼šä¼ ç»ŸLibreOfficeæ–¹å¼å‹åŠ›æµ‹è¯•")
    traditional_results = traditional_stress_test(base_files, repeat_count)

    # ç­‰å¾…ä¸€ä¸‹ï¼Œé¿å…èµ„æºå†²çª
    time.sleep(3)

    # 2. UNOæ–¹å¼å‹åŠ›æµ‹è¯•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if HAS_UNO:
        logger.info("\nğŸš€ ç¬¬äºŒé˜¶æ®µï¼šUNO APIæ–¹å¼å‹åŠ›æµ‹è¯•")
        uno_results = uno_stress_test(base_files, repeat_count)
    else:
        logger.warning("\nâš ï¸ UNO APIä¸å¯ç”¨ï¼Œè·³è¿‡UNOå‹åŠ›æµ‹è¯•")
        uno_results = {}

    # 3. ç»¼åˆå¯¹æ¯”åˆ†æ
    logger.info(f"\n{'='*120}")
    logger.info("ğŸ† ç»¼åˆæ€§èƒ½å¯¹æ¯”åˆ†æ")
    logger.info(f"{'='*120}")

    if traditional_results and uno_results:
        # æ‰¾å‡ºå„è‡ªæœ€ä½³é…ç½®
        traditional_best = max(
            ((k, v) for k, v in traditional_results.items() if "error" not in v),
            key=lambda x: x[1]["throughput"],
        )
        uno_best = max(
            ((k, v) for k, v in uno_results.items() if "error" not in v),
            key=lambda x: x[1]["throughput"],
        )

        traditional_threads, traditional_perf = traditional_best
        uno_threads, uno_perf = uno_best

        logger.info(f"ğŸ“Š æœ€ä½³æ€§èƒ½å¯¹æ¯”:")
        logger.info(
            f"   ä¼ ç»Ÿæ–¹å¼: {traditional_threads}çº¿ç¨‹, {traditional_perf['throughput']:.2f}æ–‡ä»¶/ç§’"
        )
        logger.info(f"   UNOæ–¹å¼:  {uno_threads}çº¿ç¨‹, {uno_perf['throughput']:.2f}æ–‡ä»¶/ç§’")

        # æ€§èƒ½æ¯”è¾ƒ
        speed_ratio = traditional_perf["throughput"] / uno_perf["throughput"]
        if speed_ratio > 1.2:
            logger.info(f"   ğŸ† ä¼ ç»Ÿæ–¹å¼èƒœå‡º: å¿« {speed_ratio:.2f}x")
        elif speed_ratio < 0.8:
            logger.info(f"   ğŸ† UNOæ–¹å¼èƒœå‡º: å¿« {1/speed_ratio:.2f}x")
        else:
            logger.info(f"   ğŸ¤ ä¸¤ç§æ–¹å¼æ€§èƒ½æ¥è¿‘ (æ¯”ç‡: {speed_ratio:.2f})")

        # å¹¶è¡Œæ•ˆç‡å¯¹æ¯”
        logger.info(f"\nğŸ“ˆ å¹¶è¡Œæ•ˆç‡å¯¹æ¯”:")
        for threads in [1, 4, 8, 12]:
            if threads in traditional_results and threads in uno_results:
                trad_eff = traditional_results[threads].get("efficiency", 0)
                uno_eff = uno_results[threads].get("efficiency", 0)
                trad_pct = (trad_eff / threads) * 100 if threads > 1 else 100
                uno_pct = (uno_eff / threads) * 100 if threads > 1 else 100

                logger.info(f"   {threads}çº¿ç¨‹: ä¼ ç»Ÿ{trad_pct:.1f}% vs UNO{uno_pct:.1f}%")

    logger.info(f"\nğŸ’¡ æœ€ç»ˆå»ºè®®:")
    if traditional_results:
        best_traditional = max(
            v["throughput"] for v in traditional_results.values() if "error" not in v
        )
        if HAS_UNO and uno_results:
            best_uno = max(
                v["throughput"] for v in uno_results.values() if "error" not in v
            )
            if best_traditional > best_uno * 1.2:
                logger.info("   ğŸ¯ æ¨èä½¿ç”¨ä¼ ç»ŸLibreOfficeæ–¹å¼ (use_uno=False)")
                logger.info("   âœ… ä¼ ç»Ÿæ–¹å¼åœ¨æ€§èƒ½ä¸Šæœ‰æ˜æ˜¾ä¼˜åŠ¿")
            elif best_uno > best_traditional * 1.2:
                logger.info("   ğŸ¯ æ¨èä½¿ç”¨UNO APIæ–¹å¼ (use_uno=True)")
                logger.info("   âœ… UNOæ–¹å¼åœ¨æ€§èƒ½ä¸Šæœ‰æ˜æ˜¾ä¼˜åŠ¿")
            else:
                logger.info("   ğŸ¤ ä¸¤ç§æ–¹å¼æ€§èƒ½ç›¸è¿‘ï¼Œå¯æ ¹æ®åŠŸèƒ½éœ€æ±‚é€‰æ‹©")
                logger.info("   ğŸ“ ç®€å•è½¬æ¢ç”¨ä¼ ç»Ÿæ–¹å¼ï¼Œå¤æ‚æ“ä½œç”¨UNOæ–¹å¼")
        else:
            logger.info("   ğŸ¯ æ¨èä½¿ç”¨ä¼ ç»ŸLibreOfficeæ–¹å¼ (use_uno=False)")
            logger.info("   ğŸ“ UNOä¸å¯ç”¨æˆ–æ€§èƒ½æœªæµ‹è¯•")

    return {"traditional": traditional_results, "uno": uno_results}


if __name__ == "__main__":
    # åŸºç¡€æµ‹è¯•æ–‡ä»¶åˆ—è¡¨
    base_test_files = [
        "examples/datamax.doc",
        "examples/datamax.docx"
    ]

    # æ£€æŸ¥ UNO å¯ç”¨æ€§
    if HAS_UNO:
        logger.info("âœ… UNO API å¯ç”¨")
    else:
        logger.info("âŒ UNO API ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼")

    # æä¾›æµ‹è¯•é€‰æ‹©
    import sys

    if len(sys.argv) > 1:
        test_mode = sys.argv[1].lower()

        if test_mode == "traditional":
            logger.info("\nâš¡ å¼€å§‹ä¼ ç»ŸLibreOfficeæ–¹å¼å‹åŠ›æµ‹è¯•...")
            traditional_stress_test(base_test_files, repeat_count=10)

        elif test_mode == "uno":
            if HAS_UNO:
                logger.info("\nğŸ”¥ å¼€å§‹UNO APIå¹¶è¡Œæ€§èƒ½å‹åŠ›æµ‹è¯•...")
                uno_stress_test(base_test_files, repeat_count=10)
            else:
                logger.error("âŒ UNO APIä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡ŒUNOå‹åŠ›æµ‹è¯•")

        elif test_mode == "comprehensive":
            logger.info("\nğŸ† å¼€å§‹ç»¼åˆæ€§èƒ½å¯¹æ¯”æµ‹è¯•...")
            comprehensive_stress_test(base_test_files, repeat_count=10)
            
        elif test_mode == "context":
            # æ¼”ç¤ºä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨
            if HAS_UNO:
                logger.info("\nğŸ¯ æ¼”ç¤ºä½¿ç”¨ uno_manager_context ä¸Šä¸‹æ–‡ç®¡ç†å™¨...")
                
                for file_path in base_test_files:
                    if os.path.exists(file_path):
                        # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨è‡ªåŠ¨ç®¡ç†UNOèµ„æº
                        with uno_manager_context() as manager:
                            logger.info(f"ğŸ“ ä½¿ç”¨ç®¡ç†å™¨ (ç«¯å£: {manager.port}) è½¬æ¢æ–‡ä»¶: {file_path}")
                            
                            output_path = f"{file_path}.converted.txt"
                            manager.convert_document(file_path, output_path, "txt")
                            
                            logger.info(f"âœ… è½¬æ¢å®Œæˆ: {output_path}")
                            # ç®¡ç†å™¨ä¼šè‡ªåŠ¨é‡Šæ”¾å›æ± ä¸­
                            
                logger.info("ğŸ‰ æ‰€æœ‰æ–‡ä»¶è½¬æ¢å®Œæˆï¼Œç®¡ç†å™¨å·²è‡ªåŠ¨é‡Šæ”¾")
            else:
                logger.error("âŒ UNO APIä¸å¯ç”¨")

        else:
            logger.error(f"âŒ æœªçŸ¥çš„æµ‹è¯•æ¨¡å¼: {test_mode}")
            logger.info("å¯ç”¨æ¨¡å¼: traditional, uno, comprehensive, context")
    else:
        # é»˜è®¤è¿›è¡Œä¼ ç»Ÿæ–¹å¼å‹åŠ›æµ‹è¯•
        logger.info("\nğŸ’¡ ä½¿ç”¨å‚æ•°æŒ‡å®šæµ‹è¯•æ¨¡å¼:")
        logger.info(
            "   python examples/uno_conversion_example.py traditional    # ä»…æµ‹è¯•ä¼ ç»Ÿæ–¹å¼"
        )
        logger.info(
            "   python examples/uno_conversion_example.py uno            # ä»…æµ‹è¯•UNOæ–¹å¼"
        )
        logger.info(
            "   python examples/uno_conversion_example.py comprehensive  # ç»¼åˆå¯¹æ¯”æµ‹è¯•"
        )
        logger.info(
            "   python examples/uno_conversion_example.py context        # æ¼”ç¤ºä¸Šä¸‹æ–‡ç®¡ç†å™¨"
        )
        logger.info("")
        logger.info("âš¡ é»˜è®¤è¿è¡Œä¼ ç»ŸLibreOfficeæ–¹å¼å‹åŠ›æµ‹è¯•...")
        traditional_stress_test(base_test_files, repeat_count=10)
